{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b46dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:12:49.975134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 14:12:50.585535: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-06 14:12:50.585584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-06 14:12:50.585589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-06 14:12:51 - Loading faiss with AVX2 support.\n",
      "2023-05-06 14:12:51 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import math\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "from utils.gpl_tsdae import GplTsdae\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb52787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-06 14:12:51] INFO [sentence_transformers.SentenceTransformer.__init__:66] Load pretrained SentenceTransformer: ./models/gpl/TSDAE/500000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a43486fcc4441e828836b5867906dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-06 14:12:53] INFO [models.gpl.gpl.toolkit.sbert.load_sbert:68] Set max_seq_length=350\n",
      "[2023-05-06 14:12:53] INFO [beir.retrieval.search.dense.faiss_search._load:39] Loading Faiss ID-mappings from path: ./models/gpl/embedding/TSDAE/my-index.flat.tsv\n",
      "[2023-05-06 14:12:53] INFO [beir.retrieval.search.dense.faiss_search._load:46] Loading Faiss Index from path: ./models/gpl/embedding/TSDAE/my-index.flat.faiss\n"
     ]
    }
   ],
   "source": [
    "gpl = GplTsdae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ba548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50688, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (12): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (13): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (14): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (15): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=4096, out_features=50688, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-3b\")\n",
    "model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f92d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428c90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_paper_and_save_as_txt(paper_id):\n",
    "    if os.path.exists(f\"papers_txt/{paper_id}.txt\"):\n",
    "        return\n",
    "    for try_count in range(6):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(f\"https://export.arxiv.org/pdf/{paper_id}.pdf\", f\"./temp/{paper_id}.pdf\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if try_count >= 5:\n",
    "                raise e\n",
    "            print(f\"{try_count}-retry, get {paper_id} in 10 seconds.\")\n",
    "            sleep(10)\n",
    "    pdfp = pdfplumber.open(f\"./temp/{paper_id}.pdf\")\n",
    "    full_text = '\\n'.join([page.extract_text() for page in pdfp.pages])\n",
    "    tok = sent_tokenize(full_text)\n",
    "    new_tok = []\n",
    "    for s in tok:\n",
    "        score = sum([ c not in \"+-*/=^(){}[]0123456789!@ \" and ord(c) < 128 for c in s ]) / (len(s))\n",
    "        if score >= 0.8:\n",
    "            new_tok.append(s)\n",
    "    with open(f\"papers_txt/{paper_id}.txt\", \"w\") as f:\n",
    "        f.write(' '.join(new_tok))\n",
    "    os.remove(f\"./temp/{paper_id}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab09979",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_paper_and_save_as_txt('2010.15778')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d01d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "papers_txt/1301.3834.txt\n",
      "papers_txt/1301.7521.txt\n",
      "papers_txt/1302.1727.txt\n",
      "papers_txt/1302.3921.txt\n",
      "papers_txt/1302.7145.txt\n",
      "papers_txt/1303.2579.txt\n",
      "papers_txt/1303.2580.txt\n",
      "papers_txt/1303.5751.txt\n",
      "papers_txt/1303.5768.txt\n",
      "papers_txt/1304.1235.txt\n"
     ]
    }
   ],
   "source": [
    "!find papers_txt/13* | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c40c55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is any part of p2p video streaming has same concept as bitcoin?\"\n",
    "paper_id = \"2010.15778\"\n",
    "wc = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a40217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"papers_txt/{paper_id}.txt\", \"r\") as f:\n",
    "    paper_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e903a1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ln = word_tokenize(paper_txt)\n",
    "paper_ln = [' '.join(paper_ln[i:i+wc]) for i in range(0, len(paper_ln), wc)]\n",
    "len(paper_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b29a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contextual BERT : Conditioning the Language Model Using a Global State TimoI.Denk AnaPeleteiroRamallo ZalandoSE ZalandoSE Berlin , Germany Berlin , Germany timo.denk @ zalando.de ana.peleteiro.ramallo @ zalando.de Abstract BERT is a popular language model whose main pre-training task is to ﬁll in the blank , i.e. , predicting a word that was masked out of a sentence , based on the remaining words . In some applications , however , havinganadditionalcontextcanhelpthemodelmaketherightprediction , e.g. , bytakingthedomainorthetimeofwritingintoaccount.ThismotivatesustoadvancetheBERTarchitecturebyaddingaglobalstateforconditioningonaﬁxed-sizedcontext.Wepresentourtwonovelapproachesandapplythemtoanindustryuse-case , wherewecompletefashionoutﬁtswithmissingarticles , conditionedonaspeciﬁccustomer.Anexperimentalcomparisontoothermethodsfromtheliteratureshowsthatourmethodsimprovepersonalizationsigniﬁcantly . 0202 tcO 92 1 Introduction ] LC.sc [ Sinceitspublication , theBERTmodelbyDevlinetal. ( 2019 ) hasenjoyedgreatpopularityinthenaturallanguageprocessing ( NLP ) community.Toapplythemodeltoaspeciﬁcproblem ,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ln[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bdda43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca75407c5c040d7a0e8f92e1d2c05c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = gpl.sbert.encode_corpus([ {'title': '', 'text': t} for t in paper_ln ])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c070b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afaefe4e8bf4a429fd29db8cfcaae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 526 µs, total: 13.8 ms\n",
      "Wall time: 12.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10.306605, 14.606485, 14.237837, 14.721432, 13.958326, 11.095677,\n",
       "       13.142616, 10.071073, 13.536208, 13.422296, 13.326892, 17.454824,\n",
       "        9.566545, 15.631086,  9.853745,  8.704063, 10.138346, 11.51439 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = np.sum(np.repeat(gpl.sbert.encode_queries([query]), corpus.shape[0], axis=0) * corpus, axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a87a547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 0 ns, total: 28 µs\n",
      "Wall time: 32.4 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(14.606485,\n",
       "  (1,\n",
       "   'itiscommonlypre-trainedonlargeamountsofunlabeleddata , andsubsequentlyﬁne-tunedonatargettask.Duringbothstages , themodel ’ sonlyinputisavariably-sizedsequenceofwords.Thereareuse-cases , however , wherehavinganadditionalcontextcanhelpthemodel.Consideraqueryintentclassiﬁerwhosesoleinputisauser ’ stextquery.Undertheassumptionthatusersfromdifferentagegroupsandprofessionsexpressthesameintentindifferentways , theclassiﬁerwouldbeneﬁtfromhavingaccesstothatusercontextinadditiontothequery.Alternatively , onemightconsidertrainingmultiplemodelsonseparate , agegroup-andprofession-speciﬁcsamples.However , thisapproachdoesnotscalewell , requiresmoretrainingdata , anddoesnotshareknowledgebetweenthemodels.Tothebestofourknowledge , thereisashortcomingineffectivemethodsforconditioningBERTonaﬁxed-sizedcontext.Motivatedbythis , andinspiredbythegraph-networksperspectiveonself-attentionmodels ( Battagliaetal.,2018 ) , weadvanceBERT ’ sarchitecturebyaddingaglobalstatethatenablesconditioning.Withourproposedmethods [ GS ] and [ GSU ] , wecombinetwopreviouslyindependentstreamsofwork.TheﬁrstiscenteredaroundtheideaofexplicitlyaddingaglobalstatetoBERT , albeitwithoutusingitforconditioning.ThesecondisfocusedoninjectingadditionalknowledgeintotheBERTmodel.Byusingaglobalstateforconditioning , weenabletheapplicationofBERTinarangeofuse-casesthatrequirethemodeltomakecontext-basedpredictions.Weusetheoutﬁtcompletionproblemtotesttheperformanceofournewmethods : Themodelpredictsfashionitemstocompleteanoutﬁtandhastoaccountforbothstylecoherenceandpersonalization.Forthelatter , weconditiononaﬁxed-sizedcustomerrepresentationcontaininginformationsuchascustomerage , stylepreferences , haircolor , andbodytype.Wecompareourmethodsagainsttwoothersfromtheliteratureandobservethatoursareabletoprovidemorepersonalizedpredictions . ( 2019 ) use a [ CLS ] token which is prependedtotheinputsequence ( e.g. , asentenceofnaturallanguage ) . Theassumptionisthatthemodel aggregates sentence-wide , global knowledge at the position of the [ CLS ] token . This intuition was conﬁrmedthroughattentionscoreanalysis ( Clarketal.,2019')),\n",
       " (14.721432,\n",
       "  (3,\n",
       "   'thantheotherpositions . TheresultisanincreasedperformanceondownstreamGLUEtasks . It is important to note that all related work on BERT ’ s global state does not use the global state for conditioning . Instead , thearchitecturalchangesaresolelybeingintroducedtoimprovetheperformance onnon-contextualNLPbenchmarks . Conditioning on a Context To the best of our knowledge , Wu et al . Theauthorsinjectthetargetlabel ( e.g. , positiveornegativereview ) ofsentimentdatabyaddingittothe [ CLS ] tokenembedding.Inasimilarapplication , Lietal. ( 2020 ) processthecontextseparatelyandsubsequentlycombineitwiththemodeloutputtomakeasentimentprediction.Xiaetal. ( 2020 ) conditiononricherinformation , namelyanintent , whichcanbethoughtofasataskdescriptorgiventothemodel.Theintentisrepresentedintextform , isvariablysized , andprependedtothesequence.ThisisverysimilartoawiderangeofGPT ( Radfordetal.,2019 ) applications.Chenetal. ( 2019 ) conditiononacustomer ’ svariably-sizedclickhistoryusingaTransformer ( Vaswanietal.,2017 ) .ThemostsimilartoourworkareWuetal. ( 2020 ) whopersonalizebyconcatenatingeverypositionintheinputsequencewithauserembedding–method [ C ] fromSection3.Theirapproach ,')),\n",
       " (17.454824,\n",
       "  (11,\n",
       "   'being an NLP dataset , our data resembles many of the important traits of a textual corpus : the vocabulary size is comparable to the one of word-piece vocabularies commonly used with BERT models . Fashionoutﬁtsaresimilartosentencesinthatsomearticlesappearoftentogether ( matchstyle- wise ) andothersdonot . Differentisthetypicalsequencelengthwhichrangesfromfourtoeightfashion articles , with an average length of exactly ﬁve . In contrast to sentences , outﬁts do not have an inherent order . ToaccountforthatweremovethepositionalencodingfromBERTsoittreatsitsinputasaset . Table 1 shows the results of evaluating the four different methods . We compare cross-entropy and recall @ rank ( r @ r for short ) on a randomly selected validation dataset')),\n",
       " (15.631086,\n",
       "  (13,\n",
       "   'treating the context vectorspecially . Theyattendtootherpositionsinthesequencethesamewaytheyattendtothecontext . Thesuperiorityof [ GS ] and [ GSU ] canpresumablybeexplainedbytheirexplicitarchitecturalabilitytoretrieveinformationfromtheglobalstateandthereforeeffectivelyutilizethecontextfortheirprediction . WeacknowledgethedifferencesbetweenouroutﬁtsdatasetandtypicalNLPbenchmarks . Nonetheless wehypothesizethattheeffectivenessofourmethodtranslatestoNLP.Inparticularwhenappliedtouse-casesinwhichthemodalityofcontextandsequencediffer , e.g. , forcontextscomprisedofnumericalorcategoricalmetadataaboutthetext.Thatisbecausethemodel ’ sfreedomtoreadfromthecontextseparatelyallowsittoprocessthedifferentmodalitiesofcontextandinputsequenceadequately . 5 ConclusionsandFutureWork With Contextual BERT , we presented novel ways of conditioning the BERT model . The strong perfor- manceonareal-worlduse-caseprovidesevidenceforthesuperiorityofusingaglobalstatetoinjectcon-textintotheTransformer-basedarchitecture.OurproposalenablestheeffectiveconditioningofBERT , potentiallyleadingtoimprovementsinarangeofapplicationswherecontextualinformationisrelevant.Apromisingideaforfollow-upworkistoallowforinformationtoﬂowfromthesequencetotheglobalstate.Further , itwouldbedesirabletoestablishacontextualNLPbenchmarkfortheresearchcommunitytocompeteon.ThisbenchmarkwouldtaskcompetitorswithcontextualizedNLPproblems , e.g. , socialmediaplatform-dependenttextgenerationornamedentityrecognitionformultipledomains . References Peter W. Battaglia , Jessica B. Hamrick , Victor Bapst , Alvaro Sanchez-Gonzalez , Vin´ıcius Flores Zambaldi , Ma- teuszMalinowski , AndreaTacchetti , DavidRaposo , AdamSantoro , RyanFaulkner , C¸aglarGu¨lc¸ehre , H.FrancisSong , AndrewJ.Ballard , JustinGilmer , GeorgeE.Dahl , AshishVaswani , KelseyR.Allen'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "raw_input = sorted(sorted(list(zip(scores, enumerate(paper_ln))), key=lambda x: -x[0])[:4], key=lambda x: x[1][0])\n",
    "raw_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e3fcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['itiscommonlypre-trainedonlargeamountsofunlabeleddata , andsubsequentlyﬁne-tunedonatargettask.Duringbothstages , themodel ’ sonlyinputisavariably-sizedsequenceofwords.Thereareuse-cases , however , wherehavinganadditionalcontextcanhelpthemodel.Consideraqueryintentclassiﬁerwhosesoleinputisauser ’ stextquery.Undertheassumptionthatusersfromdifferentagegroupsandprofessionsexpressthesameintentindifferentways , theclassiﬁerwouldbeneﬁtfromhavingaccesstothatusercontextinadditiontothequery.Alternatively , onemightconsidertrainingmultiplemodelsonseparate , agegroup-andprofession-speciﬁcsamples.However , thisapproachdoesnotscalewell , requiresmoretrainingdata , anddoesnotshareknowledgebetweenthemodels.Tothebestofourknowledge , thereisashortcomingineffectivemethodsforconditioningBERTonaﬁxed-sizedcontext.Motivatedbythis , andinspiredbythegraph-networksperspectiveonself-attentionmodels ( Battagliaetal.,2018 ) , weadvanceBERT ’ sarchitecturebyaddingaglobalstatethatenablesconditioning.Withourproposedmethods [ GS ] and [ GSU ] , wecombinetwopreviouslyindependentstreamsofwork.TheﬁrstiscenteredaroundtheideaofexplicitlyaddingaglobalstatetoBERT , albeitwithoutusingitforconditioning.ThesecondisfocusedoninjectingadditionalknowledgeintotheBERTmodel.Byusingaglobalstateforconditioning , weenabletheapplicationofBERTinarangeofuse-casesthatrequirethemodeltomakecontext-basedpredictions.Weusetheoutﬁtcompletionproblemtotesttheperformanceofournewmethods : Themodelpredictsfashionitemstocompleteanoutﬁtandhastoaccountforbothstylecoherenceandpersonalization.Forthelatter , weconditiononaﬁxed-sizedcustomerrepresentationcontaininginformationsuchascustomerage , stylepreferences , haircolor , andbodytype.Wecompareourmethodsagainsttwoothersfromtheliteratureandobservethatoursareabletoprovidemorepersonalizedpredictions . ( 2019 ) use a [ CLS ] token which is prependedtotheinputsequence ( e.g. , asentenceofnaturallanguage ) . Theassumptionisthatthemodel aggregates sentence-wide , global knowledge at the position of the [ CLS ] token . This intuition was conﬁrmedthroughattentionscoreanalysis ( Clarketal.,2019',\n",
       " 'thantheotherpositions . TheresultisanincreasedperformanceondownstreamGLUEtasks . It is important to note that all related work on BERT ’ s global state does not use the global state for conditioning . Instead , thearchitecturalchangesaresolelybeingintroducedtoimprovetheperformance onnon-contextualNLPbenchmarks . Conditioning on a Context To the best of our knowledge , Wu et al . Theauthorsinjectthetargetlabel ( e.g. , positiveornegativereview ) ofsentimentdatabyaddingittothe [ CLS ] tokenembedding.Inasimilarapplication , Lietal. ( 2020 ) processthecontextseparatelyandsubsequentlycombineitwiththemodeloutputtomakeasentimentprediction.Xiaetal. ( 2020 ) conditiononricherinformation , namelyanintent , whichcanbethoughtofasataskdescriptorgiventothemodel.Theintentisrepresentedintextform , isvariablysized , andprependedtothesequence.ThisisverysimilartoawiderangeofGPT ( Radfordetal.,2019 ) applications.Chenetal. ( 2019 ) conditiononacustomer ’ svariably-sizedclickhistoryusingaTransformer ( Vaswanietal.,2017 ) .ThemostsimilartoourworkareWuetal. ( 2020 ) whopersonalizebyconcatenatingeverypositionintheinputsequencewithauserembedding–method [ C ] fromSection3.Theirapproach ,',\n",
       " 'being an NLP dataset , our data resembles many of the important traits of a textual corpus : the vocabulary size is comparable to the one of word-piece vocabularies commonly used with BERT models . Fashionoutﬁtsaresimilartosentencesinthatsomearticlesappearoftentogether ( matchstyle- wise ) andothersdonot . Differentisthetypicalsequencelengthwhichrangesfromfourtoeightfashion articles , with an average length of exactly ﬁve . In contrast to sentences , outﬁts do not have an inherent order . ToaccountforthatweremovethepositionalencodingfromBERTsoittreatsitsinputasaset . Table 1 shows the results of evaluating the four different methods . We compare cross-entropy and recall @ rank ( r @ r for short ) on a randomly selected validation dataset',\n",
       " 'treating the context vectorspecially . Theyattendtootherpositionsinthesequencethesamewaytheyattendtothecontext . Thesuperiorityof [ GS ] and [ GSU ] canpresumablybeexplainedbytheirexplicitarchitecturalabilitytoretrieveinformationfromtheglobalstateandthereforeeffectivelyutilizethecontextfortheirprediction . WeacknowledgethedifferencesbetweenouroutﬁtsdatasetandtypicalNLPbenchmarks . Nonetheless wehypothesizethattheeffectivenessofourmethodtranslatestoNLP.Inparticularwhenappliedtouse-casesinwhichthemodalityofcontextandsequencediffer , e.g. , forcontextscomprisedofnumericalorcategoricalmetadataaboutthetext.Thatisbecausethemodel ’ sfreedomtoreadfromthecontextseparatelyallowsittoprocessthedifferentmodalitiesofcontextandinputsequenceadequately . 5 ConclusionsandFutureWork With Contextual BERT , we presented novel ways of conditioning the BERT model . The strong perfor- manceonareal-worlduse-caseprovidesevidenceforthesuperiorityofusingaglobalstatetoinjectcon-textintotheTransformer-basedarchitecture.OurproposalenablestheeffectiveconditioningofBERT , potentiallyleadingtoimprovementsinarangeofapplicationswherecontextualinformationisrelevant.Apromisingideaforfollow-upworkistoallowforinformationtoﬂowfromthesequencetotheglobalstate.Further , itwouldbedesirabletoestablishacontextualNLPbenchmarkfortheresearchcommunitytocompeteon.ThisbenchmarkwouldtaskcompetitorswithcontextualizedNLPproblems , e.g. , socialmediaplatform-dependenttextgenerationornamedentityrecognitionformultipledomains . References Peter W. Battaglia , Jessica B. Hamrick , Victor Bapst , Alvaro Sanchez-Gonzalez , Vin´ıcius Flores Zambaldi , Ma- teuszMalinowski , AndreaTacchetti , DavidRaposo , AdamSantoro , RyanFaulkner , C¸aglarGu¨lc¸ehre , H.FrancisSong , AndrewJ.Ballard , JustinGilmer , GeorgeE.Dahl , AshishVaswani , KelseyR.Allen']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_input = [ t[1][1] for t in raw_input]\n",
    "real_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a65cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# StableLM Tuned (Alpha version)\n",
      "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
      "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
      "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
      "- StableLM will refuse to participate in anything that could harm a human.\n",
      "Is any part of p2p video streaming has same concept as bitcoin?\n",
      "0) itiscommonlypre-trainedonlargeamountsofunlabeleddata, andsubsequentlyﬁne-tunedonatargettask.Duringbothstages, themodel ’ sonlyinputisavariably-sizedsequenceofwords.Thereareuse-cases, however, wherehavinganadditionalcontextcanhelpthemodel.Consideraqueryintentclassiﬁerwhosesoleinputisauser ’ stextquery.Undertheassumptionthatusersfromdifferentagegroupsandprofessionsexpressthesameintentindifferentways, theclassiﬁerwouldbeneﬁtfromhavingaccesstothatusercontextinadditiontothequery.Alternatively, onemightconsidertrainingmultiplemodelsonseparate, agegroup-andprofession-speciﬁcsamples.However, thisapproachdoesnotscalewell, requiresmoretrainingdata, anddoesnotshareknowledgebetweenthemodels.Tothebestofourknowledge, thereisashortcomingineffectivemethodsforconditioningBERTonaﬁxed-sizedcontext.Motivatedbythis, andinspiredbythegraph-networksperspectiveonself-attentionmodels ( Battagliaetal.,2018 ), weadvanceBERT ’ sarchitecturebyaddingaglobalstatethatenablesconditioning.Withourproposedmethods [ GS ] and [ GSU ], wecombinetwopreviouslyindependentstreamsofwork.TheﬁrstiscenteredaroundtheideaofexplicitlyaddingaglobalstatetoBERT, albeitwithoutusingitforconditioning.ThesecondisfocusedoninjectingadditionalknowledgeintotheBERTmodel.Byusingaglobalstateforconditioning, weenabletheapplicationofBERTinarangeofuse-casesthatrequirethemodeltomakecontext-basedpredictions.Weusetheoutﬁtcompletionproblemtotesttheperformanceofournewmethods : Themodelpredictsfashionitemstocompleteanoutﬁtandhastoaccountforbothstylecoherenceandpersonalization.Forthelatter, weconditiononaﬁxed-sizedcustomerrepresentationcontaininginformationsuchascustomerage, stylepreferences, haircolor, andbodytype.Wecompareourmethodsagainsttwoothersfromtheliteratureandobservethatoursareabletoprovidemorepersonalizedpredictions. ( 2019 ) use a [ CLS ] token which is prependedtotheinputsequence ( e.g., asentenceofnaturallanguage ). Theassumptionisthatthemodel aggregates sentence-wide, global knowledge at the position of the [ CLS ] token. This intuition was conﬁrmedthroughattentionscoreanalysis ( Clarketal.,2019 1) thantheotherpositions. TheresultisanincreasedperformanceondownstreamGLUEtasks. It is important to note that all related work on BERT ’ s global state does not use the global state for conditioning. Instead, thearchitecturalchangesaresolelybeingintroducedtoimprovetheperformance onnon-contextualNLPbenchmarks. Conditioning on a Context To the best of our knowledge, Wu et al. Theauthorsinjectthetargetlabel ( e.g., positiveornegativereview ) ofsentimentdatabyaddingittothe [ CLS ] tokenembedding.Inasimilarapplication, Lietal. ( 2020 ) processthecontextseparatelyandsubsequentlycombineitwiththemodeloutputtomakeasentimentprediction.Xiaetal. ( 2020 ) conditiononricherinformation, namelyanintent, whichcanbethoughtofasataskdescriptorgiventothemodel.Theintentisrepresentedintextform, isvariablysized, andprependedtothesequence.ThisisverysimilartoawiderangeofGPT ( Radfordetal.,2019 ) applications.Chenetal. ( 2019 ) conditiononacustomer ’ svariably-sizedclickhistoryusingaTransformer ( Vaswanietal.,2017 ).ThemostsimilartoourworkareWuetal. ( 2020 ) whopersonalizebyconcatenatingeverypositionintheinputsequencewithauserembedding–method [ C ] fromSection3.Theirapproach, 2) being an NLP dataset, our data resembles many of the important traits of a textual corpus : the vocabulary size is comparable to the one of word-piece vocabularies commonly used with BERT models. Fashionoutﬁtsaresimilartosentencesinthatsomearticlesappearoftentogether ( matchstyle- wise ) andothersdonot. Differentisthetypicalsequencelengthwhichrangesfromfourtoeightfashion articles, with an average length of exactly ﬁve. In contrast to sentences, outﬁts do not have an inherent order. ToaccountforthatweremovethepositionalencodingfromBERTsoittreatsitsinputasaset. Table 1 shows the results of evaluating the four different methods. We compare cross-entropy and recall @ rank ( r @ r for short ) on a randomly selected validation dataset 3) treating the context vectorspecially. Theyattendtootherpositionsinthesequencethesamewaytheyattendtothecontext. Thesuperiorityof [ GS ] and [ GSU ] canpresumablybeexplainedbytheirexplicitarchitecturalabilitytoretrieveinformationfromtheglobalstateandthereforeeffectivelyutilizethecontextfortheirprediction. WeacknowledgethedifferencesbetweenouroutﬁtsdatasetandtypicalNLPbenchmarks. Nonetheless wehypothesizethattheeffectivenessofourmethodtranslatestoNLP.Inparticularwhenappliedtouse-casesinwhichthemodalityofcontextandsequencediffer, e.g., forcontextscomprisedofnumericalorcategoricalmetadataaboutthetext.Thatisbecausethemodel ’ sfreedomtoreadfromthecontextseparatelyallowsittoprocessthedifferentmodalitiesofcontextandinputsequenceadequately. 5 ConclusionsandFutureWork With Contextual BERT, we presented novel ways of conditioning the BERT model. The strong perfor- manceonareal-worlduse-caseprovidesevidenceforthesuperiorityofusingaglobalstatetoinjectcon-textintotheTransformer-basedarchitecture.OurproposalenablestheeffectiveconditioningofBERT, potentiallyleadingtoimprovementsinarangeofapplicationswherecontextualinformationisrelevant.Apromisingideaforfollow-upworkistoallowforinformationtoﬂowfromthesequencetotheglobalstate.Further, itwouldbedesirabletoestablishacontextualNLPbenchmarkfortheresearchcommunitytocompeteon.ThisbenchmarkwouldtaskcompetitorswithcontextualizedNLPproblems, e.g., socialmediaplatform-dependenttextgenerationornamedentityrecognitionformultipledomains. References Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vin´ıcius Flores Zambaldi, Ma- teuszMalinowski, AndreaTacchetti, DavidRaposo, AdamSantoro, RyanFaulkner, C¸aglarGu¨lc¸ehre, H.FrancisSong, AndrewJ.Ballard, JustinGilmer, GeorgeE.Dahl, AshishVaswani, KelseyR.Allen <ans> The paper discusses the concept of using a large amount of unlabeled data to train a machine learning model, such as a BERT model, which is used in various applications such as text classification, sentiment analysis, and image recognition. The authors propose using a large amount of unlabeled data to train a model, and then using this data to perform inference on the target model. The paper also proposes using a large amount of unlabeled data to train a model on a specific task or problem, which can lead to better performance. The paper also discusses the challenges of using unlabeled data to train a model, such as the lack of sufficient data and the need for a large amount of computation. The paper proposes using a large amount of unlabeled data to train a model on a specific task or problem to improve performance. The paper also discusses the use of unlabeled data in natural language processing, such as text classification, sentiment analysis, and image recognition. Overall, the paper presents an overview of the concept of using unlabeled data to train a machine learning model and proposes the use of large amounts of unlabeled data to improve performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>{query}\\n{' '.join([f'{i}) {t}' for i, t in enumerate(real_input)])} <ans> <|ASSISTANT|>\"\n",
    "# print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=256,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n",
    "  top_p = 0.95, top_k = 50, early_stopping = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "883f8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = tokenizer.decode(tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20d0e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The paper discusses the concept of using a large amount of unlabeled data to train a machine learning model, such as a BERT model, which is used in various applications such as text classification, sentiment analysis, and image recognition. The authors propose using a large amount of unlabeled data to train a model, and then using this data to perform inference on the target model. The paper also proposes using a large amount of unlabeled data to train a model on a specific task or problem, which can lead to better performance. The paper also discusses the challenges of using unlabeled data to train a model, such as the lack of sufficient data and the need for a large amount of computation. The paper proposes using a large amount of unlabeled data to train a model on a specific task or problem to improve performance. The paper also discusses the use of unlabeled data in natural language processing, such as text classification, sentiment analysis, and image recognition. Overall, the paper presents an overview of the concept of using unlabeled data to train a machine learning model and proposes the use of large amounts of unlabeled data to improve performance.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.split('<ans>')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1f59c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat May  6 14:13:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:01:00.0 Off |                  Off |\n",
      "| 32%   63C    P2   221W / 300W |  47500MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:23:00.0 Off |                  Off |\n",
      "| 30%   50C    P8    23W / 300W |  21937MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc31b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# StableLM Tuned (Alpha version)\n",
      "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
      "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
      "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
      "- StableLM will refuse to participate in anything that could harm a human.\n",
      "Give 3 short search quries that You think I would like to search next from this context.\n",
      " BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. <ans> 1. \"BERT: A Lightweight Pre-training for Fine-Tuning on a Large Scale Dataset. \" \n",
      "2. \"Pre-training with BERT: Improving Personalization with Missing Articles on Fashion Clothing. \" \n",
      "3. \"LadaBERT: A Lightweight Pre-training for Fitting a Model to a Domain on a Large Scale Dataset. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>Give 3 short search quries that You think I would like to search next from this context.\\n BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. <ans> <|ASSISTANT|>\"\n",
    "# print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=256,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n",
    "  top_p = 0.95, top_k = 50, early_stopping = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afa9751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# StableLM Tuned (Alpha version)\n",
      "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
      "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
      "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
      "- StableLM will refuse to participate in anything that could harm a human.\n",
      "Gives 5 most important fact from context that relate to this query in form of number 1 to 5 list \"What is bert\"\n",
      " BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. \n",
      " <ans> 1. What is bert?\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that takes a sentence as input and predicts the next word in the sentence based on the context. It is a popular language model due to its superior performance on various natural language understanding tasks. However, BERT is limited in its ability to predict the next word in a sentence and does not have the ability to handle long-range dependencies. As a result, the model is not well-suited to handling unstructured data and it is considered a state-of-the-art model for this task.\n",
      "2. What is bert?\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that takes a sentence as input and predicts the most likely next word in the sentence based on the context. It is a popular language model for sequence-to-sequence tasks, where the goal is to predict the most probable next word in a sequence. However, it does not have the ability to handle long-range dependencies, so it cannot be used for unstructured data such as text.\n",
      "3. What is bert?\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# prompt = f\"{system_prompt}<|USER|>Gives 5 most important fact from context that relate to this query in form of number 1 to 5 list \\\"What is bert\\\"\\n BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. \\n <ans> <|ASSISTANT|>\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>Gives 5 most important fact from context that relate to this query in form of number 1 to 5 list \\\"What is bert\\\"\\n BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. \\n <ans> <|ASSISTANT|>\"\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=256,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n",
    "  top_p = 0.95, top_k = 50, early_stopping = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10466b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# StableLM Tuned (Alpha version)\n",
      "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
      "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
      "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
      "- StableLM will refuse to participate in anything that could harm a human.\n",
      "Give me the overview of “What is GAN” from given context in one paragraph. \n",
      " Generative Adversarial Network (GAN) is a well known computationally complex algorithm requiring signficiant computational resources in software implementations including large amount of data to be trained. This makes its implementation in edge devices with conventional microprocessor hardware a slow and difficult task. In this paper, we propose to accelerate the computationally intensive GAN using memristive neural networks in analog domain. We present a fully analog hardware design of Deep Convolutional GAN (DCGAN) based on CMOS-memristive convolutional and deconvolutional networks simulated using 180nm CMOS technology. The Generative Adversarial Network (GAN) is a powerful and flexible tool that can generate high-fidelity synthesized data by learning. It has seen many applications in simulating events in High Energy Physics (HEP), including simulating detector responses and physics events. However, training GANs is notoriously hard and optimizing their hyperparameters even more so. It normally requires many trial-and-error training attempts to force a stable training and reach a reasonable fidelity. Significant tuning work has to be done to achieve the accuracy required by physics analyses. This work uses the physics-agnostic and high-performance-computer-friendly hyperparameter optimization tool HYPPO to optimize and examine the sensitivities of the hyperparameters of a GAN for two independent HEP datasets. This work provides the first insights into efficiently tuning GANs for Large Hadron Collider data. We show that given proper hyperparameter tuning, we can find GANs that provide high-quality approximations of the desired quantities. We also provide guidelines for how to go about GAN architecture tuning using the analysis tools in HYPPO. Generative adversarial networks (GAN) are a class of powerful machine learning techniques, where both a generative and discriminative model are trained simultaneously. GANs have been used, for example, to successfully generate “deep fake” images. A recent trend in malware research consists of treating executables as images and employing image-based analysis techniques. In this research, we generate fake malware images using auxiliary classifier GANs (AC-GAN), and we consider the effectiveness of various techniques for classifying the resulting images. Our results indicate that the resulting multiclass classification problem is challenging, yet we can obtain strong results when restricting the problem to distinguishing between real and fake samples. While the AC-GAN generated images often appear to be very similar to real malware images, we conclude that from a deep learning perspective, the AC-GAN generated samples do not rise to the level of deep fake malware images. \n",
      " <ans> The GAN (Generative Adversarial Network) algorithm is a computationally intensive algorithm that requires significant computational resources in software implementations including large amounts of data to train. It is a powerful tool that can generate high-fidelity synthesized data by learning. However, the algorithm is notoriously hard and requires many trial-and-error training attempts to achieve a stable training and reach a reasonable fidelity. The GAN is commonly used in simulations to study the performance of algorithms, but it is also used in HEP to simulate detector responses and physics events. However, training GANs is notoriously hard and requires many trial-and-error training attempts to achieve the desired accuracy. In this paper, we propose to accelerate the computationally intensive GAN using memristive neural networks in analog domain. We present a fully analog hardware design of Deep Convolutional GAN (DCGAN) based on CMOS-memristive convolutional and deconvolutional networks simulated using 180nm CMOS technology. The Generative Adversarial Network (GAN) is a powerful and flexible tool that can generate high-fidelity synthesized data by learning. It has seen many applications in simulating events in High Energy Physics (HEP), including simulating detector responses and physics events. However, training G\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# prompt = f\"{system_prompt}<|USER|>Gives 5 most important fact from context that relate to this query in form of number 1 to 5 list \\\"What is bert\\\"\\n BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly. BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude. \\n <ans> <|ASSISTANT|>\"\n",
    "\n",
    "prompt = f\"{system_prompt}<|USER|>Give me the overview of “What is GAN” from given context in one paragraph. \\n Generative Adversarial Network (GAN) is a well known computationally complex algorithm requiring signficiant computational resources in software implementations including large amount of data to be trained. This makes its implementation in edge devices with conventional microprocessor hardware a slow and difficult task. In this paper, we propose to accelerate the computationally intensive GAN using memristive neural networks in analog domain. We present a fully analog hardware design of Deep Convolutional GAN (DCGAN) based on CMOS-memristive convolutional and deconvolutional networks simulated using 180nm CMOS technology. The Generative Adversarial Network (GAN) is a powerful and flexible tool that can generate high-fidelity synthesized data by learning. It has seen many applications in simulating events in High Energy Physics (HEP), including simulating detector responses and physics events. However, training GANs is notoriously hard and optimizing their hyperparameters even more so. It normally requires many trial-and-error training attempts to force a stable training and reach a reasonable fidelity. Significant tuning work has to be done to achieve the accuracy required by physics analyses. This work uses the physics-agnostic and high-performance-computer-friendly hyperparameter optimization tool HYPPO to optimize and examine the sensitivities of the hyperparameters of a GAN for two independent HEP datasets. This work provides the first insights into efficiently tuning GANs for Large Hadron Collider data. We show that given proper hyperparameter tuning, we can find GANs that provide high-quality approximations of the desired quantities. We also provide guidelines for how to go about GAN architecture tuning using the analysis tools in HYPPO. Generative adversarial networks (GAN) are a class of powerful machine learning techniques, where both a generative and discriminative model are trained simultaneously. GANs have been used, for example, to successfully generate “deep fake” images. A recent trend in malware research consists of treating executables as images and employing image-based analysis techniques. In this research, we generate fake malware images using auxiliary classifier GANs (AC-GAN), and we consider the effectiveness of various techniques for classifying the resulting images. Our results indicate that the resulting multiclass classification problem is challenging, yet we can obtain strong results when restricting the problem to distinguishing between real and fake samples. While the AC-GAN generated images often appear to be very similar to real malware images, we conclude that from a deep learning perspective, the AC-GAN generated samples do not rise to the level of deep fake malware images. \\n <ans> <|ASSISTANT|>\"\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=256,\n",
    "  temperature=0.7,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n",
    "  top_p = 0.95, top_k = 50, early_stopping = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947efd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde05c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tsdae]",
   "language": "python",
   "name": "conda-env-.conda-tsdae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
