{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d57779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5fd4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71937011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca1eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cuda:1'.strip('cuda:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e03913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c1169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:06:13.685113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-09 17:06:14.274367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-09 17:06:14.274410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-09 17:06:14.274415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.explain import ExplainService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53def9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ExplainService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "798d39d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_kw [(-11.410023, ('We show, propose, present, introduce', '\\nBayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems.')), (-11.302529, ('We show, propose, present, introduce', 'BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of non-linearity and high-order interactions.')), (-7.0467157, ('We show, propose, present, introduce', 'In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants.')), (-11.493725, ('We show, propose, present, introduce', 'In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree.')), (-11.360987, ('We show, propose, present, introduce', 'In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART.')), (-11.489619, ('We show, propose, present, introduce', 'Via simulation studies and real data applications, we compare MOTR-BART to its main competitors.')), (-11.502808, ('We show, propose, present, introduce', 'R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.'))]\n",
      "\n",
      "sim_kw (-7.0467157, ('We show, propose, present, introduce', 'In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants.'))\n",
      "\n",
      "sim_kw [(9.240786, ('What is bart', '\\nBayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems.')), (4.868445, ('What is bart', 'BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of non-linearity and high-order interactions.')), (-0.6574012, ('What is bart', 'In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree.')), (1.5999217, ('What is bart', 'In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART.')), (-2.6192646, ('What is bart', 'Via simulation studies and real data applications, we compare MOTR-BART to its main competitors.')), (-2.8632922, ('What is bart', 'R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.'))]\n",
      "\n",
      "sim_kw (9.240786, ('What is bart', '\\nBayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems.'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Learning: Bayesian Additive Regression Trees: A Efficient Model Tree-Based Machine Learning Method for Regressiv Tree s for classification problems']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expsss = exps.explain2(\"What is bart\",\n",
    "\"\"\"\n",
    "Bayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems. BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of non-linearity and high-order interactions. In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants. In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree. In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART. Via simulation studies and real data applications, we compare MOTR-BART to its main competitors. R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.\n",
    "\"\"\", verbose=True)\n",
    "expsss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02fa7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees <extra_id_0>: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: <extra_id_0>: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: <extra_id_0> Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees Factor Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees <extra_id_0>: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees Empirical Learning: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Tree Replay: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees: Bayesian Additive Regression Trees'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(expsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7cae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0b4fd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems.',\n",
       " 'The idea is that if you have a bunch of data, and you want to make a prediction about the distribution of the data, you can use a tree to predict the distribution.',\n",
       " 'The problem is that there are a lot of variables in the data that can influence the distribution, so you need a way to predict how the distribution will change over time.',\n",
       " 'In BART, you use a piecewise linear function to estimate the distribution over time, and then you use another piecewise function to calculate the distribution at each node of the tree.',\n",
       " \"This way, you don't have to worry about non-linearity and high-order interactions.\",\n",
       " 'In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART).',\n",
       " 'In this extension, we use pieceswise linear functions at node levels instead of piecewise constants.',\n",
       " 'This means that instead of using a linear function at every node, we only need to use piecewise functions at the nodes that are correlated with each other.',\n",
       " 'In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expsss = exps.explain(\"What is bart\",\n",
    "\"\"\"\n",
    "Bayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems. BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of non-linearity and high-order interactions. In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants. In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree. In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART. Via simulation studies and real data applications, we compare MOTR-BART to its main competitors. R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.\n",
    "\"\"\")\n",
    "expsss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a4b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5cd7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'order': 1,\n",
       "  'sentence': 'Bayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems.',\n",
       "  'value': 0.9945840239524841},\n",
       " {'order': 2,\n",
       "  'sentence': 'The idea is that if you have a bunch of data, and you want to make a prediction about the distribution of the data, you can use a tree to predict the distribution.',\n",
       "  'value': 1.2002091542839821e-09},\n",
       " {'order': 3,\n",
       "  'sentence': 'The problem is that there are a lot of variables in the data that can influence the distribution, so you need a way to predict how the distribution will change over time.',\n",
       "  'value': 1.1487346629479589e-09},\n",
       " {'order': 4,\n",
       "  'sentence': 'In BART, you use a piecewise linear function to estimate the distribution over time, and then you use another piecewise function to calculate the distribution at each node of the tree.',\n",
       "  'value': 0.001427985611371696},\n",
       " {'order': 5,\n",
       "  'sentence': \"This way, you don't have to worry about non-linearity and high-order interactions.\",\n",
       "  'value': 1.176745700881554e-09},\n",
       " {'order': 6,\n",
       "  'sentence': 'In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART).',\n",
       "  'value': 0.0035102313850075006},\n",
       " {'order': 7,\n",
       "  'sentence': 'In this extension, we use pieceswise linear functions at node levels instead of piecewise constants.',\n",
       "  'value': 1.0879196432611593e-09},\n",
       " {'order': 8,\n",
       "  'sentence': 'This means that instead of using a linear function at every node, we only need to use piecewise functions at the nodes that are correlated with each other.',\n",
       "  'value': 1.1442518044191274e-09},\n",
       " {'order': 9,\n",
       "  'sentence': 'In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART.',\n",
       "  'value': 0.0004778115835506469}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps.highlight(\"What is bart\", expsss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3620c71",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6f3c4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, MT5ForConditionalGeneration\n",
    "from sentence_transformers import CrossEncoder\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "\n",
    "pattern = r'[0-9]'\n",
    "\n",
    "class ExplainServiceWithOverview(ExplainService):\n",
    "\n",
    "    def _gen_questions(self, documents, num_return_sequences=5):\n",
    "        t5_encoded_inputs = self.tokenizer_T5.batch_encode_plus(\n",
    "            documents, return_tensors='pt', padding=True)\n",
    "        generated_question_sequences = self.model_T5.generate(\n",
    "            input_ids=t5_encoded_inputs['input_ids'],\n",
    "            attention_mask=t5_encoded_inputs['attention_mask'],\n",
    "            max_length=64,\n",
    "            do_sample=True, \n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_return_sequences\n",
    "        )\n",
    "        decoded_sequences = self.tokenizer_T5.batch_decode(\n",
    "            generated_question_sequences, skip_special_tokens=True)\n",
    "        return decoded_sequences\n",
    "    \n",
    "    def _calc_similiarity(self, query, sentences):\n",
    "        if type(query) != type([]):\n",
    "            query = [query]\n",
    "        sentence_combinations = [(q, s) for q in query for s in sentences]\n",
    "        similarity_scores = self.model_CE.predict(sentence_combinations)\n",
    "        return list(zip(similarity_scores, sentence_combinations))\n",
    "    \n",
    "    def _gen_question(self,\n",
    "                 query, \n",
    "                 documents,\n",
    "                 num_return_sequences,\n",
    "                 similarity_threshold, \n",
    "                 min_pass,\n",
    "                 template_questions,\n",
    "                 verbose):\n",
    "        questions = self._gen_questions(documents, num_return_sequences)\n",
    "        questions = sorted(self._calc_similiarity(query, questions), \n",
    "                         key=lambda x: -x[0])\n",
    "        if verbose: print(\"questions:\\n\", questions)\n",
    "        \n",
    "        cut = 0\n",
    "        for score, question in questions:\n",
    "            if cut >= min_pass and score < similarity_threshold:\n",
    "                break\n",
    "            cut += 1\n",
    "        questions = [q[1][1] for q in questions[:cut]]\n",
    "        if verbose: print(\"cut at:\", cut)\n",
    "            \n",
    "        questions_template_sim = self._calc_similiarity(\n",
    "            [f\"{t}\" for t in template_questions], questions)\n",
    "        if verbose: print(\"questions_template_sim:\\n\", questions_template_sim)\n",
    "            \n",
    "        final_questions = []\n",
    "        for i in range(0, len(questions_template_sim), len(questions)):\n",
    "            final_questions.append(\n",
    "                max(\n",
    "                    questions_template_sim[i:i+len(questions)],\n",
    "                    key=lambda x: x[0]\n",
    "                )[1][1]\n",
    "            )\n",
    "        if verbose: print(\"final_questions:\\n\", final_questions)\n",
    "        return final_questions\n",
    "    \n",
    "    def _generate_answer(self, question, documents):\n",
    "        conditioned_doc = \"<P> \" + \" <P> \".join(documents)\n",
    "        query_and_docs = \"question: {} context: {}\".format(question, conditioned_doc)\n",
    "        \n",
    "        model_input = self.tokenizer_lfqa(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        generated_answers_encoded = self.model_lfqa.generate(\n",
    "            input_ids=model_input[\"input_ids\"].to(self.device),\n",
    "            attention_mask=model_input[\"attention_mask\"].to(self.device),\n",
    "            min_length=64,\n",
    "            max_length=256,\n",
    "            do_sample=False, \n",
    "            early_stopping=False,\n",
    "            num_beams=8,\n",
    "            temperature=12,\n",
    "            top_k=None,\n",
    "            top_p=0.97,\n",
    "            eos_token_id=self.tokenizer_lfqa.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=1)\n",
    "        \n",
    "        ans = self.tokenizer_lfqa.batch_decode(generated_answers_encoded, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        if \"I'm not sure\" in ans[0] :\n",
    "            preprocess_text = ans[0].strip().replace(\"\\n\",\"\")\n",
    "            t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "            tokenized_text = self.T5_arxiv_tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\", add_special_tokens=True).to(self.device)\n",
    "            summary_ids = self.T5_arxiv_model.generate(\n",
    "                tokenized_text,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "                min_length=30,\n",
    "                max_length=100,\n",
    "                early_stopping=True)\n",
    "            ans = [self.T5_arxiv_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids][0]\n",
    "            encoded = self.T5_arxiv_tokenizer.encode_plus(ans, add_special_tokens=True, return_tensors='pt')\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "\n",
    "            # Generaing 20 sequences with maximum length set to 5\n",
    "            outputs = self.T5_arxiv_model.generate(\n",
    "                input_ids=input_ids, \n",
    "                num_beams=30,\n",
    "                num_return_sequences=10,\n",
    "                max_length=30)\n",
    "            \n",
    "            try:\n",
    "                _0_index = ans.index('<extra_id_0>')\n",
    "                _result_prefix = ans[:_0_index]\n",
    "                _result_suffix = ans[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "\n",
    "                results = [self._filter(o, _result_prefix, _result_suffix) for o in outputs]\n",
    "                ans = [results[0]]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        sentences = self.tokenizer_sentence.tokenize(ans[0])\n",
    "        for i in range(len(sentences)):\n",
    "            sen = sentences[i]\n",
    "            find = re.findall(pattern, sen)\n",
    "            if len(find) > 12 :\n",
    "                sentences[i] = ''\n",
    "        sentences = [f\"(q: {question})\", *sentences]\n",
    "        return ' '.join(sentences)\n",
    "        \n",
    "    def overview(self, \n",
    "                 query, \n",
    "                 documents,\n",
    "                 num_return_sequences=5,\n",
    "                 similarity_threshold=2, \n",
    "                 min_pass=5,\n",
    "                 template_questions=[\"What is this?\", \"What is it use for?\"],\n",
    "                 verbose=False):\n",
    "        questions = self._gen_question(\n",
    "            query, \n",
    "            documents,\n",
    "            num_return_sequences,\n",
    "            similarity_threshold, \n",
    "            min_pass,\n",
    "            template_questions,\n",
    "            verbose\n",
    "        )\n",
    "        return [self._generate_answer(q, documents) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b89323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, MT5ForConditionalGeneration\n",
    "from sentence_transformers import CrossEncoder\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "\n",
    "pattern = r'[0-9]'\n",
    "\n",
    "class ExplainServiceWithOverview(ExplainService):\n",
    "\n",
    "    def _gen_questions(self, documents, num_return_sequences=5):\n",
    "        t5_encoded_inputs = self.tokenizer_T5.batch_encode_plus(\n",
    "            documents, return_tensors='pt', padding=True)\n",
    "        generated_question_sequences = self.model_T5.generate(\n",
    "            input_ids=t5_encoded_inputs['input_ids'],\n",
    "            attention_mask=t5_encoded_inputs['attention_mask'],\n",
    "            max_length=64,\n",
    "            do_sample=True, \n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_return_sequences\n",
    "        )\n",
    "        decoded_sequences = self.tokenizer_T5.batch_decode(\n",
    "            generated_question_sequences, skip_special_tokens=True)\n",
    "        return decoded_sequences\n",
    "    \n",
    "    def _calc_similiarity(self, query, sentences):\n",
    "        if type(query) != type([]):\n",
    "            query = [query]\n",
    "        sentence_combinations = [(q, s) for q in query for s in sentences]\n",
    "        similarity_scores = self.model_CE.predict(sentence_combinations)\n",
    "        return list(zip(similarity_scores, sentence_combinations))\n",
    "    \n",
    "    def _gen_question(self,\n",
    "                 query, \n",
    "                 documents,\n",
    "                 num_return_sequences,\n",
    "                 template_questions,\n",
    "                 verbose):\n",
    "        questions = self._gen_questions(documents, num_return_sequences)\n",
    "        if verbose: print(\"questions:\\n\", questions)\n",
    "        \n",
    "        question_sim = self._calc_similiarity([query, *template_questions], questions)\n",
    "        query_sim, template_question_sim = question_sim[:len(questions)], question_sim[len(questions):]\n",
    "            \n",
    "        final_questions = []\n",
    "        for i in range(0, len(template_question_sim), len(questions)):\n",
    "            final_questions.append(\n",
    "                max(\n",
    "                    [(q[0] + qt[0], qt[1])\n",
    "                     for q, qt in zip(query_sim, template_question_sim[i:i+len(questions)])],\n",
    "                    key=lambda x: x[0]\n",
    "                )[1][1]\n",
    "            )\n",
    "        if verbose: print(\"final_questions:\\n\", final_questions)\n",
    "        return final_questions\n",
    "    \n",
    "    def _generate_answer(self, question, documents):\n",
    "        conditioned_doc = \"<P> \" + \" <P> \".join(documents)\n",
    "        query_and_docs = \"question: {} context: {}\".format(question, conditioned_doc)\n",
    "        \n",
    "        model_input = self.tokenizer_lfqa(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        generated_answers_encoded = self.model_lfqa.generate(\n",
    "            input_ids=model_input[\"input_ids\"].to(self.device),\n",
    "            attention_mask=model_input[\"attention_mask\"].to(self.device),\n",
    "            min_length=64,\n",
    "            max_length=256,\n",
    "            do_sample=False, \n",
    "            early_stopping=False,\n",
    "            num_beams=8,\n",
    "            temperature=12,\n",
    "            top_k=None,\n",
    "            top_p=0.97,\n",
    "            eos_token_id=self.tokenizer_lfqa.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=1)\n",
    "        \n",
    "        ans = self.tokenizer_lfqa.batch_decode(generated_answers_encoded, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        if \"I'm not sure\" in ans[0] :\n",
    "            preprocess_text = ans[0].strip().replace(\"\\n\",\"\")\n",
    "            t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "            tokenized_text = self.T5_arxiv_tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\", add_special_tokens=True).to(self.device)\n",
    "            summary_ids = self.T5_arxiv_model.generate(\n",
    "                tokenized_text,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "                min_length=30,\n",
    "                max_length=100,\n",
    "                early_stopping=True)\n",
    "            ans = [self.T5_arxiv_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids][0]\n",
    "            encoded = self.T5_arxiv_tokenizer.encode_plus(ans, add_special_tokens=True, return_tensors='pt')\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "\n",
    "            # Generaing 20 sequences with maximum length set to 5\n",
    "            outputs = self.T5_arxiv_model.generate(\n",
    "                input_ids=input_ids, \n",
    "                num_beams=30,\n",
    "                num_return_sequences=10,\n",
    "                max_length=30)\n",
    "            \n",
    "            try:\n",
    "                _0_index = ans.index('<extra_id_0>')\n",
    "                _result_prefix = ans[:_0_index]\n",
    "                _result_suffix = ans[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "\n",
    "                results = [self._filter(o, _result_prefix, _result_suffix) for o in outputs]\n",
    "                ans = [results[0]]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        sentences = self.tokenizer_sentence.tokenize(ans[0])\n",
    "        for i in range(len(sentences)):\n",
    "            sen = sentences[i]\n",
    "            find = re.findall(pattern, sen)\n",
    "            if len(find) > 12 :\n",
    "                sentences[i] = ''\n",
    "        sentences = [f\"(q: {question})\", *sentences]\n",
    "        return ' '.join(sentences)\n",
    "        \n",
    "    def overview(self, \n",
    "                 query, \n",
    "                 documents,\n",
    "                 num_return_sequences=5,\n",
    "                 min_pass=5,\n",
    "                 template_questions=[\"What is this?\", \"What is it use for?\"],\n",
    "                 verbose=False):\n",
    "        questions = self._gen_question(\n",
    "            query,\n",
    "            documents,\n",
    "            num_return_sequences,\n",
    "            template_questions,\n",
    "            verbose\n",
    "        )\n",
    "#         return [self._generate_answer(q, documents) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "82b2dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExplainServiceWithOverview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b4a0ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions:\n",
      " ['what is nns', 'what is nns in infant', 'what is non-nutritive sucking?', 'what is nns', 'what is nns in psychology', 'how is nns monitored?', 'what is nns', 'what is the sensory sensory signal to reflexively stop sucking?', 'what is an nns', 'when is a baby not sucking', 'what is betacovingt', 'what is betcovington', 'what is lstm in computer language', 'what is lstm', 'what is fastparse', 'what is fastparse', 'what is fastparse', 'which algorithm is the lys fastparse', 'what is fastparse used for?', 'when was lstm used', 'what is the sials network', 'what is sils network', 'what is silsm', 'which scenario is an example of collision-prone behavior?', 'what is sils network', 'which is the underlying behavior of a collision trajectory?', 'which network is used to detect collisions', 'when does an intersection become unsafe', 'where do traffic collisions happen', 'what is sials network', 'what is src and lstm rnn', 'when to use src method', 'what method of testing is used to detect concussion', 'what is the correct method for diagnosing a concussion', 'why is sport concussion considered an unreliable test', 'what is src test', 'what is src?', 'what is the best method to identify src', 'what is the lstm rnn approach for detecting a concussion?', 'can athletes report symptoms without causing injury', 'what is the term lstm used to predict', 'what is lstm', 'how does a network of traffic sensors work', 'why do cities need a traffic sensors', 'lstm meaning', 'what network of traffic sensors are used to detect emission?', 'which lstm is the best model to use for traffic prediction?', 'what is the traffic pattern for valencia ai', 'how does an artificial intelligence do the traffic?', 'how does the lstm predict the flow of traffic in valencia']\n",
      "final_questions:\n",
      " ['what is silsm', 'what is fastparse used for?']\n",
      "\n",
      "\n",
      "overview:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "q = \"Why LSTM is suck?\"\n",
    "result = requests.get(\"http://172.18.0.12:3001/search\", dict(query=q, limit=5)).json()['result']\n",
    "print(\"\\n\\noverview:\\n\", exp.overview(\"What is BERT?\", [r['abstract'] for r in result], num_return_sequences=10, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8945274",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"NP-Complete problems\"\n",
    "result = requests.get(\"http://172.18.0.12:3001/search\", dict(query=q, limit=10)).json()['result']\n",
    "print(\"\\n\\noverview:\\n\", exps.overview(\"Indexing Algorithms\", [r['abstract'] for r in result], similarity_threshold=2, num_return_sequences=1, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e426666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tsdae]",
   "language": "python",
   "name": "conda-env-.conda-tsdae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
