{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e342373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from utils.FastChat.fastchat.conversation import conv_templates, SeparatorStyle\n",
    "from fastchat.conversation import conv_templates, SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bb312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"utils/FastChat/repositories/GPTQ-for-LLaMa\")\n",
    "sys.path.insert(0, \"utils/FastChat/repositories/GPTQ-for-LLaMa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dc0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LlmPipeline():\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_stream(self, tokenizer, model, params, device,\n",
    "                        context_len=2048, stream_interval=2):\n",
    "        \"\"\"Adapted from fastchat/serve/model_worker.py::generate_stream\"\"\"\n",
    "\n",
    "        prompt = params[\"prompt\"]\n",
    "        l_prompt = len(prompt)\n",
    "        temperature = float(params.get(\"temperature\", 1.0))\n",
    "    #     max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n",
    "        max_new_tokens = 1024\n",
    "        stop_str = params.get(\"stop\", None)\n",
    "\n",
    "        input_ids = tokenizer(prompt).input_ids\n",
    "        output_ids = list(input_ids)\n",
    "\n",
    "        max_src_len = context_len - max_new_tokens - 8\n",
    "        input_ids = input_ids[-max_src_len:]\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            if i == 0:\n",
    "                out = model(\n",
    "                    torch.as_tensor([input_ids], device=device), use_cache=True)\n",
    "                logits = out.logits\n",
    "                past_key_values = out.past_key_values\n",
    "            else:\n",
    "                attention_mask = torch.ones(\n",
    "                    1, past_key_values[0][0].shape[-2] + 1, device=device)\n",
    "                out = model(input_ids=torch.as_tensor([[token]], device=device),\n",
    "                            use_cache=True,\n",
    "                            attention_mask=attention_mask,\n",
    "                            past_key_values=past_key_values)\n",
    "                logits = out.logits\n",
    "                past_key_values = out.past_key_values\n",
    "\n",
    "            last_token_logits = logits[0][-1]\n",
    "            if temperature < 1e-4:\n",
    "                token = int(torch.argmax(last_token_logits))\n",
    "            else:\n",
    "                probs = torch.softmax(last_token_logits / temperature, dim=-1)\n",
    "                token = int(torch.multinomial(probs, num_samples=1))\n",
    "\n",
    "            output_ids.append(token)\n",
    "\n",
    "            if token == tokenizer.eos_token_id:\n",
    "                stopped = True\n",
    "            else:\n",
    "                stopped = False\n",
    "\n",
    "            if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n",
    "                output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                pos = output.rfind(stop_str, l_prompt)\n",
    "                if pos != -1:\n",
    "                    output = output[:pos]\n",
    "                    stopped = True\n",
    "                yield output\n",
    "\n",
    "            if stopped:\n",
    "                break\n",
    "\n",
    "        del past_key_values\n",
    "\n",
    "    def __init__(self, device='cuda', model_name='anon8231489123/vicuna-13b-GPTQ-4bit-128g', \n",
    "                 num_gpus='1', wbits=4, temperature=0.7, max_new_tokens=512, conv_template='v1'):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.num_gpus = num_gpus\n",
    "        self.wbits = wbits\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.conv_template = conv_template\n",
    "        \n",
    "        # Model\n",
    "        if device == \"cuda\":\n",
    "            kwargs = {\"torch_dtype\": torch.float16}\n",
    "            if num_gpus == \"auto\":\n",
    "                kwargs[\"device_map\"] = \"auto\"\n",
    "            else:\n",
    "                num_gpus = int(num_gpus)\n",
    "                if num_gpus != 1:\n",
    "                    kwargs.update({\n",
    "                        \"device_map\": \"auto\",\n",
    "                        \"max_memory\": {i: \"13GiB\" for i in range(num_gpus)},\n",
    "                    })\n",
    "        elif device == \"cpu\":\n",
    "            kwargs = {}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid device: {device}\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if wbits > 0:\n",
    "            from fastchat.serve.load_gptq_model import load_quantized\n",
    "\n",
    "            print(\"Loading GPTQ quantized model...\")\n",
    "            model = load_quantized(model_name)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                low_cpu_mem_usage=True, **kwargs)\n",
    "\n",
    "        if device == \"cuda\" and num_gpus == 1:\n",
    "            model.cuda()\n",
    "            \n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "    def infer(self, inp, conv=None):\n",
    "        if conv is None:\n",
    "            conv = conv_templates[self.conv_template].copy()\n",
    "        \n",
    "        conv.append_message(conv.roles[0], inp) #Appending Input\n",
    "        conv.append_message(conv.roles[1], None) #Appending Assitant:, Waiting for the right response from the model\n",
    "        prompt = conv.get_prompt()\n",
    "        params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"stop\": conv.sep if conv.sep_style == SeparatorStyle.SINGLE else conv.sep2,\n",
    "        }\n",
    "        \n",
    "        pre = 0\n",
    "        for outputs in self.generate_stream(self.tokenizer, self.model, params, self.device):\n",
    "            outputs = outputs[len(prompt) + 1:].strip()\n",
    "            outputs = outputs.split(\" \")\n",
    "        \n",
    "        conv.messages[-1][-1] = \" \".join(outputs)\n",
    "        return \" \".join(outputs[pre:]), conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e969b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPTQ quantized model...\n",
      "Loading model ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "llm = LlmPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930e6cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:51:58.201235: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 13:51:58.737767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-11 13:51:58.737814: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-11 13:51:58.737820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google that is designed to understand natural language processing (NLP) tasks. BERT is based on the transformer architecture, which was introduced in a paper by Vaswani et al. in 2017, and it uses a bidirectional approach to process text, which means it processes text in both forward and backward directions.\\nBERT is trained on a large corpus of text and is capable of understanding the context and relationships between words in a sentence, which makes it well-suited for a wide range of NLP tasks such as sentiment analysis, question answering, and text classification.\\nThe BERT model has been made available as an open-source library, which has enabled a lot of research and development in the field of NLP, and it has been used in many applications such as chatbots, virtual assistants, and language translation.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, conv = llm.infer(\"What is bert\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07128b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT works by using a multi-layer transformer architecture to process text in both forward and backward directions. The transformer architecture is a type of neural network that uses self-attention mechanisms to process sequences of data, such as words in a sentence.\\nWhen processing a sentence, BERT first encodes the words in the sentence into a set of numerical features using a series of feedforward layers. These features are then passed through a multi-head self-attention layer, which allows the model to understand the relationships between words in the sentence. This is followed by a layer of bias and activation functions, which allow the model to make predictions and perform tasks such as classification and sentiment analysis.\\nThe bidirectional processing in BERT allows it to understand the context of words in a sentence, which is important for tasks such as sentiment analysis and question answering. The model can also be fine-tuned for specific tasks by adjusting the layers and parameters of the model, and by training it on a specific dataset.\\nOverall, BERT is a powerful and flexible language model that is capable of understanding complex natural language patterns and performing a wide range of NLP tasks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, conv = llm.infer('How it work', conv)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bb7acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation(system=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\", roles=('Human', 'Assistant'), messages=[['Human', 'What are the key differences between renewable and non-renewable energy sources?'], ['Assistant', 'Renewable energy sources are those that can be replenished naturally in a relatively short amount of time, such as solar, wind, hydro, geothermal, and biomass. Non-renewable energy sources, on the other hand, are finite and will eventually be depleted, such as coal, oil, and natural gas. Here are some key differences between renewable and non-renewable energy sources:\\n1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable energy sources are finite and will eventually run out.\\n2. Environmental impact: Renewable energy sources have a much lower environmental impact than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, and other negative effects.\\n3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically have lower operational costs than non-renewable sources.\\n4. Reliability: Renewable energy sources are often more reliable and can be used in more remote locations than non-renewable sources.\\n5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different situations and needs, while non-renewable sources are more rigid and inflexible.\\n6. Sustainability: Renewable energy sources are more sustainable over the long term, while non-renewable sources are not, and their depletion can lead to economic and social instability.\\n'], ['Human', 'What is bert'], ['Assistant', 'BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google that is designed to understand natural language processing (NLP) tasks. BERT is based on the transformer architecture, which was introduced in a paper by Vaswani et al. in 2017, and it uses a bidirectional approach to process text, which means it processes text in both forward and backward directions.\\nBERT is trained on a large corpus of text and is capable of understanding the context and relationships between words in a sentence, which makes it well-suited for a wide range of NLP tasks such as sentiment analysis, question answering, and text classification.\\nThe BERT model has been made available as an open-source library, which has enabled a lot of research and development in the field of NLP, and it has been used in many applications such as chatbots, virtual assistants, and language translation.'], ['Human', 'How it work'], ['Assistant', 'BERT works by using a multi-layer transformer architecture to process text in both forward and backward directions. The transformer architecture is a type of neural network that uses self-attention mechanisms to process sequences of data, such as words in a sentence.\\nWhen processing a sentence, BERT first encodes the words in the sentence into a set of numerical features using a series of feedforward layers. These features are then passed through a multi-head self-attention layer, which allows the model to understand the relationships between words in the sentence. This is followed by a layer of bias and activation functions, which allow the model to make predictions and perform tasks such as classification and sentiment analysis.\\nThe bidirectional processing in BERT allows it to understand the context of words in a sentence, which is important for tasks such as sentiment analysis and question answering. The model can also be fine-tuned for specific tasks by adjusting the layers and parameters of the model, and by training it on a specific dataset.\\nOverall, BERT is a powerful and flexible language model that is capable of understanding complex natural language patterns and performing a wide range of NLP tasks.']], offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, skip_next=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41f8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"  COVID-19 is an acute respiratory disease that has been classified as a\\npandemic by the World Health Organization. Characterization of this disease is\\nstill in its early stages. However, it is known to have high mortality rates,\\nparticularly among individuals with preexisting medical conditions. Creating\\nmodels to identify individuals who are at the greatest risk for severe\\ncomplications due to COVID-19 will be useful for outreach campaigns to help\\nmitigate the disease's worst effects. While information specific to COVID-19 is\\nlimited, a model using complications due to other upper respiratory infections\\ncan be used as a proxy to help identify those individuals who are at the\\ngreatest risk. We present the results for three models predicting such\\ncomplications, with each model increasing predictive effectiveness at the\\nexpense of ease of implementation.\\n\",\n",
    "    \"  The ongoing COVID-19 pandemic has had far-reaching effects throughout\\nsociety, and science is no exception. The scale, speed, and breadth of the\\nscientific community's COVID-19 response has lead to the emergence of new\\nresearch literature on a remarkable scale -- as of October 2020, over 81,000\\nCOVID-19 related scientific papers have been released, at a rate of over 250\\nper day. This has created a challenge to traditional methods of engagement with\\nthe research literature; the volume of new research is far beyond the ability\\nof any human to read, and the urgency of response has lead to an increasingly\\nprominent role for pre-print servers and a diffusion of relevant research\\nacross sources. These factors have created a need for new tools to change the\\nway scientific literature is disseminated. COVIDScholar is a knowledge portal\\ndesigned with the unique needs of the COVID-19 research community in mind,\\nutilizing NLP to aid researchers in synthesizing the information spread across\\nthousands of emergent research articles, patents, and clinical trials into\\nactionable insights and new knowledge. The search interface for this corpus,\\nhttps://covidscholar.org, now serves over 2000 unique users weekly. We present\\nalso an analysis of trends in COVID-19 research over the course of 2020.\\n\",\n",
    "    \"  The COVID-19 epidemic is considered as the global health crisis of the whole\\nsociety and the greatest challenge mankind faced since World War Two.\\nUnfortunately, the fake news about COVID-19 is spreading as fast as the virus\\nitself. The incorrect health measurements, anxiety, and hate speeches will have\\nbad consequences on people's physical health, as well as their mental health in\\nthe whole world. To help better combat the COVID-19 fake news, we propose a new\\nfake news detection dataset MM-COVID(Multilingual and Multidimensional COVID-19\\nFake News Data Repository). This dataset provides the multilingual fake news\\nand the relevant social context. We collect 3981 pieces of fake news content\\nand 7192 trustworthy information from English, Spanish, Portuguese, Hindi,\\nFrench and Italian, 6 different languages. We present a detailed and\\nexploratory analysis of MM-COVID from different perspectives and demonstrate\\nthe utility of MM-COVID in several potential applications of COVID-19 fake news\\nstudy on multilingual and social media.\\n\",\n",
    "    \"  We present COVID-SEE, a system for medical literature discovery based on the\\nconcept of information exploration, which builds on several distinct text\\nanalysis and natural language processing methods to structure and organise\\ninformation in publications, and augments search by providing a visual overview\\nsupporting exploration of a collection to identify key articles of interest. We\\ndeveloped this system over COVID-19 literature to help medical professionals\\nand researchers explore the literature evidence, and improve findability of\\nrelevant information. COVID-SEE is available at http://covid-see.com.\\n\",\n",
    "    \"  Coronavirus disease (COVID-19) is an infectious disease, which is caused by\\nthe SARS-CoV-2 virus. Due to the growing literature on COVID-19, it is hard to\\nget precise, up-to-date information about the virus. Practitioners, front-line\\nworkers, and researchers require expert-specific methods to stay current on\\nscientific knowledge and research findings. However, there are a lot of\\nresearch papers being written on the subject, which makes it hard to keep up\\nwith the most recent research. This problem motivates us to propose the design\\nof the COVID-19 Search Engine (CO-SE), which is an algorithmic system that\\nfinds relevant documents for each query (asked by a user) and answers complex\\nquestions by searching a large corpus of publications. The CO-SE has a\\nretriever component trained on the TF-IDF vectorizer that retrieves the\\nrelevant documents from the system. It also consists of a reader component that\\nconsists of a Transformer-based model, which is used to read the paragraphs and\\nfind the answers related to the query from the retrieved documents. The\\nproposed model has outperformed previous models, obtaining an exact match ratio\\nscore of 71.45% and a semantic answer similarity score of 78.55%. It also\\noutperforms other benchmark datasets, demonstrating the generalizability of the\\nproposed approach.\\n\"\n",
    "]\n",
    "query = \"What is covid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4002d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 401 ms, total: 30.2 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "context = \" \".join(documents).replace(\"\\n\", \" \")\n",
    "prompt = f'Give me the overview of \"{query}\" from given context in one paragraph.\\nContext: {context}'\n",
    "ans, _ = llm.infer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b56138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COVID-19 has been declared a global health emergency by the World Health Organization and has affected many aspects of society, including the scientific community. The scale and speed of the pandemic have led to a rapid increase in COVID-19 related scientific literature, with over 81,000 publications released as of October 2020. This has created a need for new tools to help researchers stay up to date with the latest findings and make sense of the vast amount of information available.\\n\\nOne solution to this problem is the COVID-19 Search Engine (CO-SE), a system that uses natural language processing and machine learning to search a large corpus of publications and provide relevant results to users. The CO-SE consists of a retriever component trained on a TF-IDF vectorizer and a reader component that uses a Transformer-based model to find answers to user queries. The system has been tested on a dataset of 1000 queries and has achieved a high accuracy, outperforming previous models and benchmark datasets.\\n\\nAnother solution is the COVID-19 Knowledge Portal, an online platform that uses natural language processing to synthesize information from thousands of research articles, patents, and clinical trials into actionable insights and new knowledge. The portal serves over 2000 unique users weekly and provides a search interface for the COVID-19 research corpus.\\n\\nIn addition to these solutions, the COVID-19 fake news detection dataset (MM-COVID) has been proposed to help combat the spread of misinformation about the virus. The dataset provides multilingual fake news articles and relevant social context and has been used to develop models that can detect fake news with high accuracy.\\n\\nFinally, the COVID-SEE system has been developed to help medical professionals and researchers explore the COVID-19 literature and improve findability of relevant information. The system uses natural language processing and text analysis methods to structure and organize information in publications and provide a visual overview to support exploration of the collection.\\n\\nIn conclusion, the COVID-19 pandemic has had a significant impact on society and the scientific community, leading to a rapid increase in COVID-19 related scientific literature. To address this challenge, new tools and solutions are needed to help researchers stay up to date with the latest findings and make sense of the vast amount of information available. The COVID-19 Search Engine, COVID-19 Knowledge Portal, COVID-19 fake news detection dataset, and COVID-SEE system are some of the solutions that have been developed to address these challenges and help improve the response to the COVID-19 pandemic.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21264971",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"  COVID-19 is an acute respiratory disease that has been classified as a\\npandemic by the World Health Organization. Characterization of this disease is\\nstill in its early stages. However, it is known to have high mortality rates,\\nparticularly among individuals with preexisting medical conditions. Creating\\nmodels to identify individuals who are at the greatest risk for severe\\ncomplications due to COVID-19 will be useful for outreach campaigns to help\\nmitigate the disease's worst effects. While information specific to COVID-19 is\\nlimited, a model using complications due to other upper respiratory infections\\ncan be used as a proxy to help identify those individuals who are at the\\ngreatest risk. We present the results for three models predicting such\\ncomplications, with each model increasing predictive effectiveness at the\\nexpense of ease of implementation.\\n\",\n",
    "    \"  The ongoing COVID-19 pandemic has had far-reaching effects throughout\\nsociety, and science is no exception. The scale, speed, and breadth of the\\nscientific community's COVID-19 response has lead to the emergence of new\\nresearch literature on a remarkable scale -- as of October 2020, over 81,000\\nCOVID-19 related scientific papers have been released, at a rate of over 250\\nper day. This has created a challenge to traditional methods of engagement with\\nthe research literature; the volume of new research is far beyond the ability\\nof any human to read, and the urgency of response has lead to an increasingly\\nprominent role for pre-print servers and a diffusion of relevant research\\nacross sources. These factors have created a need for new tools to change the\\nway scientific literature is disseminated. COVIDScholar is a knowledge portal\\ndesigned with the unique needs of the COVID-19 research community in mind,\\nutilizing NLP to aid researchers in synthesizing the information spread across\\nthousands of emergent research articles, patents, and clinical trials into\\nactionable insights and new knowledge. The search interface for this corpus,\\nhttps://covidscholar.org, now serves over 2000 unique users weekly. We present\\nalso an analysis of trends in COVID-19 research over the course of 2020.\\n\",\n",
    "    \"  The COVID-19 epidemic is considered as the global health crisis of the whole\\nsociety and the greatest challenge mankind faced since World War Two.\\nUnfortunately, the fake news about COVID-19 is spreading as fast as the virus\\nitself. The incorrect health measurements, anxiety, and hate speeches will have\\nbad consequences on people's physical health, as well as their mental health in\\nthe whole world. To help better combat the COVID-19 fake news, we propose a new\\nfake news detection dataset MM-COVID(Multilingual and Multidimensional COVID-19\\nFake News Data Repository). This dataset provides the multilingual fake news\\nand the relevant social context. We collect 3981 pieces of fake news content\\nand 7192 trustworthy information from English, Spanish, Portuguese, Hindi,\\nFrench and Italian, 6 different languages. We present a detailed and\\nexploratory analysis of MM-COVID from different perspectives and demonstrate\\nthe utility of MM-COVID in several potential applications of COVID-19 fake news\\nstudy on multilingual and social media.\\n\",\n",
    "    \"  We present COVID-SEE, a system for medical literature discovery based on the\\nconcept of information exploration, which builds on several distinct text\\nanalysis and natural language processing methods to structure and organise\\ninformation in publications, and augments search by providing a visual overview\\nsupporting exploration of a collection to identify key articles of interest. We\\ndeveloped this system over COVID-19 literature to help medical professionals\\nand researchers explore the literature evidence, and improve findability of\\nrelevant information. COVID-SEE is available at http://covid-see.com.\\n\",\n",
    "    \"  Coronavirus disease (COVID-19) is an infectious disease, which is caused by\\nthe SARS-CoV-2 virus. Due to the growing literature on COVID-19, it is hard to\\nget precise, up-to-date information about the virus. Practitioners, front-line\\nworkers, and researchers require expert-specific methods to stay current on\\nscientific knowledge and research findings. However, there are a lot of\\nresearch papers being written on the subject, which makes it hard to keep up\\nwith the most recent research. This problem motivates us to propose the design\\nof the COVID-19 Search Engine (CO-SE), which is an algorithmic system that\\nfinds relevant documents for each query (asked by a user) and answers complex\\nquestions by searching a large corpus of publications. The CO-SE has a\\nretriever component trained on the TF-IDF vectorizer that retrieves the\\nrelevant documents from the system. It also consists of a reader component that\\nconsists of a Transformer-based model, which is used to read the paragraphs and\\nfind the answers related to the query from the retrieved documents. The\\nproposed model has outperformed previous models, obtaining an exact match ratio\\nscore of 71.45% and a semantic answer similarity score of 78.55%. It also\\noutperforms other benchmark datasets, demonstrating the generalizability of the\\nproposed approach.\\n\"\n",
    "]\n",
    "query = \"What is covid\"\n",
    "\n",
    "%%time\n",
    "context = \" \".join(documents).replace(\"\\n\", \" \")\n",
    "prompt = f'Give me the overview of \"{query}\" from given context in one paragraph.\\nContext: {context}'\n",
    "ans, _ = llm.infer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af1955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tsdae]",
   "language": "python",
   "name": "conda-env-.conda-tsdae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
